{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf187f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, csv, re, time, os\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories if they don't exist\"\"\"\n",
    "    Path(\"../results/csv\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"../results/csv/big_csv\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05564ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_remoteok(keyword):\n",
    "    \"\"\"\n",
    "    Scrapes RemoteOK for jobs matching a keyword.\n",
    "    Example keyword: 'machine-learning'\n",
    "    \"\"\"\n",
    "    url = f\"https://remoteok.com/remote-{keyword}-jobs\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        jobs = []\n",
    "        for row in soup.select(\"tr.job\"):\n",
    "            title = row.select_one(\"h2[itemprop='title']\")\n",
    "            company = row.select_one(\"h3[itemprop='name']\")\n",
    "            desc = row.select_one(\"td.description\")  # may be short preview\n",
    "            link = row.get(\"data-url\")\n",
    "\n",
    "            # extract clean text\n",
    "            title = title.get_text(strip=True) if title else \"N/A\"\n",
    "            company = company.get_text(strip=True) if company else \"N/A\"\n",
    "            desc = desc.get_text(strip=True) if desc else \"N/A\"\n",
    "            job_url = f\"https://remoteok.com{link}\" if link else \"N/A\"\n",
    "\n",
    "            # Expanded skills list based on the image\n",
    "            skills_list = [\n",
    "                \"Python\", \"TensorFlow\", \"PyTorch\", \"SQL\", \"AWS\", \"GCP\", \"Azure\", \n",
    "                \"LangChain\", \"OpenCV\", \"RPA\", \"ChatGPT\", \"Generative AI\", \"LLM\", \n",
    "                \"GPT-4\", \"Prompt Engineering\", \"Scikit-learn\", \"XGBoost\", \"BERT\", \n",
    "                \"NLP\", \"Text Analytics\", \"Text Classification\", \"Image Recognition\", \n",
    "                \"YOLO\", \"Vision Transformer\", \"Automation\", \"Robotics\", \"ROS\",\n",
    "                \"Responsible AI\", \"AI governance\", \"AI regulation\", \"Machine Learning\",\n",
    "                \"Deep Learning\", \"Computer Vision\", \"Natural Language Processing\",\n",
    "                \"Data Science\", \"ML\", \"AI\", \"Neural Networks\", \"Reinforcement Learning\",\n",
    "                \"Data Mining\", \"Big Data\", \"Hadoop\", \"Spark\", \"Kubernetes\", \"Docker\"\n",
    "            ]\n",
    "            found_skills = [s for s in skills_list if re.search(rf\"\\b{s}\\b\", desc, re.IGNORECASE)]\n",
    "\n",
    "            jobs.append({\n",
    "                \"title\": title,\n",
    "                \"company\": company,\n",
    "                \"url\": job_url,\n",
    "                \"description\": desc[:200],  # keep first 200 chars\n",
    "                \"skills\": \", \".join(found_skills),\n",
    "                \"keyword\": keyword  # Add keyword for tracking\n",
    "            })\n",
    "\n",
    "        return jobs\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error scraping {keyword}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407980ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_individual_csv(jobs, keyword):\n",
    "    \"\"\"Save jobs to individual CSV file\"\"\"\n",
    "    filename = f\"../results/csv/remoteok_{keyword}.csv\"\n",
    "    \n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\",\"company\",\"url\",\"description\",\"skills\",\"keyword\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(jobs)\n",
    "    \n",
    "    print(f\"‚úÖ Saved {len(jobs)} {keyword} jobs to {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_csvs(keywords):\n",
    "    \"\"\"Combine all individual CSV files into one big CSV\"\"\"\n",
    "    all_jobs = []\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        filename = f\"../results/csv/remoteok_{keyword}.csv\"\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            try:\n",
    "                with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    for row in reader:\n",
    "                        all_jobs.append(row)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error reading {filename}: {e}\")\n",
    "    \n",
    "    # Save combined CSV\n",
    "    big_csv_filename = \"../results/csv/big_csv/remoteok_all_jobs.csv\"\n",
    "    with open(big_csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\",\"company\",\"url\",\"description\",\"skills\",\"keyword\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_jobs)\n",
    "    \n",
    "    print(f\"‚úÖ Combined {len(all_jobs)} jobs from all keywords into {big_csv_filename}\")\n",
    "    return big_csv_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88077c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the scraping process\"\"\"\n",
    "    setup_directories()\n",
    "    \n",
    "    # Expanded list of keywords based on the image\n",
    "    keywords = [\n",
    "        \"machine-learning\", \"nlp\", \"ai\", \"robo\", \"generative\", \"ml\", \"automation\",\n",
    "        \"chatgpt\", \"gpt-4\", \"prompt-engineering\", \"scikit-learn\", \"xgboost\", \"bert\",\n",
    "        \"text-analytics\", \"text-classification\", \"image-recognition\", \"opencv\", \"yolo\",\n",
    "        \"vision-transformer\", \"robotics\", \"ros\", \"responsible-ai\", \"ai-governance\",\n",
    "        \"ai-regulation\", \"deep-learning\", \"computer-vision\", \"natural-language-processing\",\n",
    "        \"data-science\", \"neural-networks\", \"reinforcement-learning\", \"data-mining\",\n",
    "        \"big-data\", \"hadoop\", \"spark\", \"kubernetes\", \"docker\"\n",
    "    ]\n",
    "    \n",
    "    total_jobs = 0\n",
    "    all_files = []\n",
    "    \n",
    "    # Scrape each keyword and save individual CSV files\n",
    "    for i, keyword in enumerate(keywords):\n",
    "        print(f\"üîç Scraping {keyword} jobs ({i+1}/{len(keywords)})...\")\n",
    "        jobs = scrape_remoteok(keyword)\n",
    "        \n",
    "        if jobs:\n",
    "            filename = save_individual_csv(jobs, keyword)\n",
    "            all_files.append(filename)\n",
    "            total_jobs += len(jobs)\n",
    "            \n",
    "        # Add delay to be respectful to the server\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Combine all CSV files into one big CSV\n",
    "    if total_jobs > 0:\n",
    "        combine_all_csvs(keywords)\n",
    "        print(f\"üéâ Total jobs scraped across all keywords: {total_jobs}\")\n",
    "    else:\n",
    "        print(\"‚ùå No jobs were scraped from any keyword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
